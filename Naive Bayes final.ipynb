{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the code for using Naive Bayes method to classify truthful vs. deceptive hotel reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing Libraries*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reading the files and doing the only data preprocessing we used, applying lowercase to all words*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reading(fname1, fname2):\n",
    "    file=open(fname1)\n",
    "    \n",
    "    true=file.readlines()\n",
    "    \n",
    "    for i in range(len(true)):\n",
    "        \n",
    "        true[i]=true[i].lower()\n",
    "        \n",
    "    file=open(fname2)\n",
    "    \n",
    "    fake=file.readlines()\n",
    "    \n",
    "    for i in range(len(fake)):\n",
    "        \n",
    "        fake[i]=fake[i].lower()\n",
    "        \n",
    "    return(true,fake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A function to combine the truthful and fake reviews into dataframes with the appropriate labels, for both training and validation sets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Combining(list1,list2):\n",
    "\n",
    "    df_reviews_train_t=pd.DataFrame()\n",
    "\n",
    "    df_reviews_train_t=pd.DataFrame(list1)\n",
    "\n",
    "    df_reviews_train_t.columns={'Reviews'}\n",
    "\n",
    "    df_reviews_train_t['Label']=0\n",
    "\n",
    "   \n",
    "\n",
    "    df_reviews_train_f=pd.DataFrame()\n",
    "\n",
    "    df_reviews_train_f=pd.DataFrame(list2)\n",
    "\n",
    "    df_reviews_train_f.columns={'Reviews'}\n",
    "\n",
    "    df_reviews_train_f['Label']=1\n",
    "\n",
    "   \n",
    "\n",
    "    Final=[df_reviews_train_t,df_reviews_train_f ]\n",
    "\n",
    "    result=pd.concat(Final)\n",
    "\n",
    "    return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Set\n",
    "Reviews_train_t,Reviews_train_f=Reading('truthful.txt','deceptive.txt')\n",
    "\n",
    "DF_Reviews_train=Combining(Reviews_train_t,Reviews_train_f)\n",
    "\n",
    "#Validation Set\n",
    "Reviews_val_t,Reviews_val_f=Reading('truthfulval.txt','deceptiveval.txt')\n",
    "\n",
    "DF_Reviews_val=Combining(Reviews_val_t,Reviews_val_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Function to read test file and apply lowecase on it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reading_test(fname1):\n",
    "\n",
    "    file=open(fname1)\n",
    "\n",
    "    test=file.readlines()\n",
    "\n",
    "    for i in range(len(test)):\n",
    "\n",
    "        test[i]=test[i].lower()\n",
    "\n",
    "    return(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_test=Reading_test('test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the packages and methods required for Naive Bayes Implementation. We'll be trying out two type of NB, Multinomial and Bernoulli.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit training data\n",
    "vect = CountVectorizer().fit(DF_Reviews_train['Reviews'])\n",
    "\n",
    "#Transform training, validation and test sets\n",
    "\n",
    "X_train_vectorized = vect.transform(DF_Reviews_train['Reviews'])\n",
    "X_Val_vectorized = vect.transform(DF_Reviews_val['Reviews'])\n",
    "test_vectorized= vect.transform(Reviews_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try both types of Naive Bayes on the Validation set and see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Accuracy on Validation with Alpha 1: 91.40625\n",
      "Multinomial Accuracy on Validation with Alpha 0.1: 90.234375\n"
     ]
    }
   ],
   "source": [
    "#Multinomial \n",
    "\n",
    "#Alpha = 1\n",
    "\n",
    "clfrNB = MultinomialNB(alpha = 1)\n",
    "\n",
    "clfrNB.fit(X_train_vectorized,DF_Reviews_train['Label'] )\n",
    "\n",
    "preds = clfrNB.predict(vect.transform(DF_Reviews_val['Reviews']))\n",
    "\n",
    " \n",
    "score =  accuracy_score(DF_Reviews_val['Label'], preds)\n",
    "\n",
    "print('Multinomial Accuracy on Validation with Alpha 1: '+ str(score*100))\n",
    "\n",
    "#Alpha = 0.1\n",
    "\n",
    "clfr1NB = MultinomialNB(alpha = 0.1)\n",
    "\n",
    "clfr1NB.fit(X_train_vectorized,DF_Reviews_train['Label'] )\n",
    "\n",
    "preds = clfr1NB.predict(vect.transform(DF_Reviews_val['Reviews']))\n",
    "\n",
    " \n",
    "score =  accuracy_score(DF_Reviews_val['Label'], preds)\n",
    "\n",
    "print('Multinomial Accuracy on Validation with Alpha 0.1: '+ str(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Accuracy on Validation with Alpha 1: 87.890625\n",
      "Bernoulli Accuracy on Validation with Alpha 0.1: 88.671875\n"
     ]
    }
   ],
   "source": [
    "#Bernoulli\n",
    "\n",
    "#Alpha = 1\n",
    "\n",
    "BerNB = BernoulliNB(alpha = 1)\n",
    "\n",
    "BerNB.fit(X_train_vectorized,DF_Reviews_train['Label'] )\n",
    "\n",
    "preds = BerNB.predict(vect.transform(DF_Reviews_val['Reviews']))\n",
    "\n",
    " \n",
    "score =  accuracy_score(DF_Reviews_val['Label'], preds)\n",
    "\n",
    "print('Bernoulli Accuracy on Validation with Alpha 1: '+ str(score*100))\n",
    "\n",
    "#Alpha = 0.1\n",
    "\n",
    "BernNB = BernoulliNB(alpha = 0.1)\n",
    "\n",
    "BernNB.fit(X_train_vectorized,DF_Reviews_train['Label'] )\n",
    "\n",
    "preds = BernNB.predict(vect.transform(DF_Reviews_val['Reviews']))\n",
    "\n",
    " \n",
    "score =  accuracy_score(DF_Reviews_val['Label'], preds)\n",
    "\n",
    "print('Bernoulli Accuracy on Validation with Alpha 0.1: '+ str(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing submission file from both Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha = 0.1\n",
    "test_preds_Bern = BernNB.predict(test_vectorized)\n",
    "#Aplha = 1\n",
    "test_preds_Bern_1 = BerNB.predict(test_vectorized)\n",
    "#Alpha = 1\n",
    "test_preds_Multi_1 = clfrNB.predict(test_vectorized)\n",
    "#Alpha = 0.1\n",
    "test_preds_Multi = clfr1NB.predict(test_vectorized)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting them to CSV's, we found out that Bernoulli NB(Alpha = 0.1) had the highest accuracy on Kaggle of 91.406%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0\n",
      " 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1\n",
      " 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
      " 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_preds_Bern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given below in the layout we used to convert them to CSV's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test_data.index,\n",
    "                       'Prediction': test_preds})\n",
    "                       \n",
    "                       \n",
    "                       \n",
    "output.to_csv('submission_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
