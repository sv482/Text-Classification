{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the implementation of n-gram-based language model for the Opinion Spam Classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libaries and Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Open the Files\n",
    "\n",
    "d_train = open(\"deceptive.txt\")\n",
    "t_train=open(\"truthful.txt\")\n",
    "d_val = open(\"deceptiveval.txt\")\n",
    "t_val = open(\"truthfulval.txt\")\n",
    "test=open(\"test.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get individual words from the file into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individualtokens(file):\n",
    "    allreview = file.readlines()\n",
    "    reviews = []\n",
    "    for word in allreview: \n",
    "         reviews.append(word.split(' '))\n",
    "    return reviews\n",
    "\n",
    "\n",
    "truth_train=individualtokens(t_train)\n",
    "deceptive_train=individualtokens(d_train)\n",
    "truth_val=individualtokens(t_val)\n",
    "deceptive_val=individualtokens(d_val)\n",
    "testdata=individualtokens(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    newdata=[]\n",
    "    for sentence in data:\n",
    "         words = [word.lower() for word in sentence]\n",
    "         newdata.append(words)\n",
    "    return newdata    \n",
    "            \n",
    "            \n",
    "      \n",
    "\n",
    "updated_truth_train = lowercase(truth_train)\n",
    "updated_deceptive_train=lowercase(deceptive_train)\n",
    "updated_truth_val=lowercase(truth_val)\n",
    "updated_deceptive_val=lowercase(deceptive_val)\n",
    "updated_test=lowercase(testdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams and Unigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unigram(review):\n",
    "    UnigramList={}\n",
    "    for sentence in review:\n",
    "        for word in sentence:\n",
    "            if word not in UnigramList:\n",
    "                UnigramList[word]=0\n",
    "            UnigramList[word]+=1\n",
    "    return UnigramList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "truth_train_unigrams = Unigram(updated_truth_train)\n",
    "deceptive_train_unigrams = Unigram(updated_deceptive_train)\n",
    "\n",
    "#Counts\n",
    "\n",
    "def UnigramCount(review):\n",
    "    count=0\n",
    "    for sentence in review:\n",
    "        for word in sentence:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "truth_train_Unigram_Count = UnigramCount(updated_truth_train)\n",
    "deceptive_train_Unigram_Count= UnigramCount(updated_deceptive_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bigram(review):\n",
    "    BigramList = {}\n",
    "    for sentence in review: \n",
    "        combination = [(sentence[i], sentence[i + 1]) for i in range(len(sentence) - 1)]\n",
    "        for  duo in combination:\n",
    "            Bigram = duo[0] + \" \" + duo[1]\n",
    "            if Bigram not in BigramList:\n",
    "                BigramList[Bigram]=0\n",
    "            BigramList[Bigram] += 1\n",
    "    return BigramList   \n",
    "\n",
    "\n",
    "            \n",
    "truth_train_bigrams= Bigram(updated_truth_train)\n",
    "deceptive_train_bigrams = Bigram(updated_deceptive_train) \n",
    "\n",
    "#Counts\n",
    "\n",
    "def BigramCount(BigramList,review):\n",
    "    count = sum(BigramList.values())\n",
    "    return count\n",
    "    \n",
    "truth_train_Bigram_Count = BigramCount(truth_train_bigrams,updated_truth_train)\n",
    "deceptive_train_Bigram_Count= BigramCount(deceptive_train_bigrams,updated_deceptive_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram and Bigram Probabilities(Log Probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnigramProbability(Unigram,count):\n",
    "    UnigramProbability={}\n",
    "    for i in Unigram:\n",
    "        UnigramProbability[i] = np.log(Unigram[i]/count)\n",
    "    return UnigramProbability\n",
    "\n",
    "    \n",
    "truth_train_Unigram_Probability = UnigramProbability(truth_train_unigrams,truth_train_Unigram_Count)  \n",
    "deceptive_train_Unigram_Probability = UnigramProbability(deceptive_train_unigrams,deceptive_train_Unigram_Count)\n",
    "\n",
    "\n",
    "def BigramProbability(Bigram,Unigram):\n",
    "    BigramProbability={}\n",
    "    for i in Bigram:\n",
    "        BigramProbability[i] = np.log(Bigram[i]/Unigram[i.split(\" \")[0]])\n",
    "    return BigramProbability    \n",
    "\n",
    "truth_train_Bigram_Probability = BigramProbability(truth_train_bigrams,truth_train_unigrams)\n",
    "deceptive_train_Bigram_Probability = BigramProbability(deceptive_train_bigrams,deceptive_train_unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique values in both the training sets for Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_train_Dict_count = len(truth_train_unigrams)\n",
    "deceptive_train_Dict_count = len(deceptive_train_unigrams)\n",
    "truth_train_Dict_bigram_count=len(truth_train_bigrams)\n",
    "deceptive_train_Dict_bigram_count=len(deceptive_train_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown Word Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnknownWordHandler(review,unigram,n):   \n",
    "    for review in sentence:\n",
    "        for i in range(0,len(review)):\n",
    "            if review[i] in unigram:\n",
    "                if unigram[review[i]] < n:\n",
    "                    review[i] = '<UNK>'\n",
    "            if review[i] not in unigram:\n",
    "                review[i] = '<UNK>'\n",
    "                unigram[review[i]] += 1\n",
    "    return unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing. After experimenting with values, we took k=1 for the test set and k=0.1 for the validation set. Also, we chose Unigram for smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdditiveSmoothingUnigram(review,Unigram,count,distinct,k):\n",
    "    SmoothedUnigramProb={}\n",
    "    for sentence in review:\n",
    "        for word in sentence:\n",
    "            if word in Unigram.keys():\n",
    "                SmoothedUnigramProb[word] = np.log((k+ Unigram[word])/(count+(k*distinct)))\n",
    "            else:\n",
    "                SmoothedUnigramProb[word] = np.log(k/(count+(k*distinct)))\n",
    "    return SmoothedUnigramProb   \n",
    "\n",
    "\n",
    "#On Truthful Validation Set\n",
    "truth_val_Unigrams_Probability = AdditiveSmoothingUnigram(updated_truth_val,truth_train_unigrams,truth_train_Unigram_Count,truth_train_Dict_count,0.1)\n",
    "deceptive_val_Unigrams_Probability = AdditiveSmoothingUnigram(updated_truth_val,deceptive_train_unigrams,deceptive_train_Unigram_Count,deceptive_train_Dict_count,0.1)\n",
    "\n",
    "#On Deceptive Validation Set\n",
    "truth_val_Unigrams_Probability1 = AdditiveSmoothingUnigram(updated_deceptive_val,truth_train_unigrams,truth_train_Unigram_Count,truth_train_Dict_count,0.1)\n",
    "deceptive_val_Unigrams_Probability1 = AdditiveSmoothingUnigram(updated_deceptive_val,deceptive_train_unigrams,deceptive_train_Unigram_Count,deceptive_train_Dict_count,0.1)\n",
    "\n",
    "#On Test Set\n",
    "truth_test_Unigrams_Probabilty=AdditiveSmoothingUnigram(updated_test,truth_train_unigrams,truth_train_Unigram_Count,truth_train_Dict_count,1)\n",
    "deceptive_test_Unigrams_Probability = AdditiveSmoothingUnigram(updated_test,deceptive_train_unigrams,deceptive_train_Unigram_Count,deceptive_train_Dict_count,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnigramProbabilityforPerplexity(review,dictionary):\n",
    "    total = 0\n",
    "    for word in review:\n",
    "        total = total + dictionary[word]\n",
    "    return total    \n",
    "        \n",
    "  \n",
    "\n",
    "def Perplexity(review,smootheddictionary,n_gram_probability):\n",
    "    return np.exp((-1/len(review))*n_gram_probability(review,smootheddictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the accuracies for both the validation sets based on the Perplexity. We return the lower perplexity class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Validation Set Accuracy: 94.53125\n",
      "Deceptive Validation Set Accuracy: 92.96875\n"
     ]
    }
   ],
   "source": [
    "#Initializing Dict to store values of class after perplexity calculation\n",
    "dict_truthful_output={}\n",
    "dict_deceptive_output={}\n",
    "\n",
    "#Truthful Validation Set\n",
    "for i in range(0,len(updated_truth_val)):\n",
    "    if Perplexity(updated_truth_val[i],truth_val_Unigrams_Probability,UnigramProbabilityforPerplexity) < Perplexity(updated_truth_val[i],deceptive_val_Unigrams_Probability,UnigramProbabilityforPerplexity):\n",
    "        dict_truthful_output[i] = 1\n",
    "    else:\n",
    "        dict_truthful_output[i] = 0\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "Accuracy_truth_Unigram_Val = (sum(dict_truthful_output.values())/len(updated_truth_val))\n",
    "\n",
    "print('Truth Validation Set Accuracy: ' + str(Accuracy_truth_Unigram_Val*100))     \n",
    "\n",
    "\n",
    "#Deceptive Validation Set\n",
    "\n",
    "for i in range(0,len(updated_deceptive_val)):\n",
    "    if Perplexity(updated_deceptive_val[i],truth_val_Unigrams_Probability1,UnigramProbabilityforPerplexity) > Perplexity(updated_deceptive_val[i],deceptive_val_Unigrams_Probability1,UnigramProbabilityforPerplexity):\n",
    "        dict_deceptive_output[i] = 1\n",
    "    else:\n",
    "         dict_deceptive_output[i]=0\n",
    "        \n",
    "  \n",
    "    \n",
    "Accuracy_deceptive_Unigram_Val = (sum(dict_deceptive_output.values())/len(updated_deceptive_val))\n",
    "print('Deceptive Validation Set Accuracy: ' + str(Accuracy_deceptive_Unigram_Val*100))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead on our test set :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0\n",
      "1,0\n",
      "2,1\n",
      "3,1\n",
      "4,0\n",
      "5,0\n",
      "6,1\n",
      "7,1\n",
      "8,1\n",
      "9,0\n",
      "10,1\n",
      "11,1\n",
      "12,1\n",
      "13,1\n",
      "14,1\n",
      "15,1\n",
      "16,1\n",
      "17,0\n",
      "18,1\n",
      "19,0\n",
      "20,0\n",
      "21,1\n",
      "22,1\n",
      "23,1\n",
      "24,1\n",
      "25,1\n",
      "26,1\n",
      "27,1\n",
      "28,1\n",
      "29,0\n",
      "30,1\n",
      "31,0\n",
      "32,0\n",
      "33,0\n",
      "34,1\n",
      "35,0\n",
      "36,1\n",
      "37,0\n",
      "38,1\n",
      "39,1\n",
      "40,0\n",
      "41,1\n",
      "42,1\n",
      "43,1\n",
      "44,1\n",
      "45,1\n",
      "46,0\n",
      "47,1\n",
      "48,1\n",
      "49,1\n",
      "50,0\n",
      "51,1\n",
      "52,0\n",
      "53,1\n",
      "54,1\n",
      "55,1\n",
      "56,0\n",
      "57,1\n",
      "58,0\n",
      "59,1\n",
      "60,1\n",
      "61,1\n",
      "62,0\n",
      "63,1\n",
      "64,0\n",
      "65,1\n",
      "66,0\n",
      "67,0\n",
      "68,0\n",
      "69,1\n",
      "70,1\n",
      "71,1\n",
      "72,0\n",
      "73,0\n",
      "74,0\n",
      "75,1\n",
      "76,1\n",
      "77,1\n",
      "78,1\n",
      "79,1\n",
      "80,0\n",
      "81,0\n",
      "82,0\n",
      "83,0\n",
      "84,0\n",
      "85,0\n",
      "86,0\n",
      "87,1\n",
      "88,0\n",
      "89,0\n",
      "90,1\n",
      "91,0\n",
      "92,0\n",
      "93,0\n",
      "94,1\n",
      "95,1\n",
      "96,1\n",
      "97,1\n",
      "98,1\n",
      "99,0\n",
      "100,0\n",
      "101,0\n",
      "102,0\n",
      "103,1\n",
      "104,0\n",
      "105,0\n",
      "106,0\n",
      "107,1\n",
      "108,1\n",
      "109,1\n",
      "110,0\n",
      "111,1\n",
      "112,0\n",
      "113,1\n",
      "114,1\n",
      "115,0\n",
      "116,0\n",
      "117,1\n",
      "118,1\n",
      "119,1\n",
      "120,0\n",
      "121,0\n",
      "122,1\n",
      "123,0\n",
      "124,0\n",
      "125,1\n",
      "126,0\n",
      "127,1\n",
      "128,0\n",
      "129,1\n",
      "130,0\n",
      "131,0\n",
      "132,1\n",
      "133,1\n",
      "134,0\n",
      "135,0\n",
      "136,0\n",
      "137,1\n",
      "138,0\n",
      "139,1\n",
      "140,0\n",
      "141,1\n",
      "142,0\n",
      "143,0\n",
      "144,1\n",
      "145,1\n",
      "146,0\n",
      "147,1\n",
      "148,1\n",
      "149,0\n",
      "150,0\n",
      "151,1\n",
      "152,0\n",
      "153,1\n",
      "154,1\n",
      "155,1\n",
      "156,1\n",
      "157,1\n",
      "158,1\n",
      "159,1\n",
      "160,0\n",
      "161,0\n",
      "162,1\n",
      "163,0\n",
      "164,0\n",
      "165,1\n",
      "166,1\n",
      "167,1\n",
      "168,1\n",
      "169,0\n",
      "170,1\n",
      "171,1\n",
      "172,0\n",
      "173,0\n",
      "174,0\n",
      "175,1\n",
      "176,1\n",
      "177,0\n",
      "178,1\n",
      "179,0\n",
      "180,1\n",
      "181,1\n",
      "182,0\n",
      "183,1\n",
      "184,1\n",
      "185,1\n",
      "186,0\n",
      "187,0\n",
      "188,1\n",
      "189,1\n",
      "190,1\n",
      "191,0\n",
      "192,1\n",
      "193,1\n",
      "194,1\n",
      "195,1\n",
      "196,0\n",
      "197,0\n",
      "198,1\n",
      "199,1\n",
      "200,1\n",
      "201,1\n",
      "202,1\n",
      "203,1\n",
      "204,1\n",
      "205,1\n",
      "206,0\n",
      "207,0\n",
      "208,1\n",
      "209,1\n",
      "210,0\n",
      "211,1\n",
      "212,0\n",
      "213,0\n",
      "214,1\n",
      "215,1\n",
      "216,1\n",
      "217,1\n",
      "218,0\n",
      "219,1\n",
      "220,0\n",
      "221,0\n",
      "222,0\n",
      "223,0\n",
      "224,0\n",
      "225,0\n",
      "226,0\n",
      "227,1\n",
      "228,0\n",
      "229,1\n",
      "230,0\n",
      "231,1\n",
      "232,0\n",
      "233,0\n",
      "234,0\n",
      "235,1\n",
      "236,1\n",
      "237,1\n",
      "238,0\n",
      "239,0\n",
      "240,1\n",
      "241,0\n",
      "242,1\n",
      "243,1\n",
      "244,0\n",
      "245,1\n",
      "246,1\n",
      "247,1\n",
      "248,1\n",
      "249,1\n",
      "250,0\n",
      "251,1\n",
      "252,1\n",
      "253,1\n",
      "254,0\n",
      "255,1\n",
      "256,1\n",
      "257,0\n",
      "258,0\n",
      "259,0\n",
      "260,1\n",
      "261,0\n",
      "262,1\n",
      "263,0\n",
      "264,1\n",
      "265,1\n",
      "266,1\n",
      "267,0\n",
      "268,0\n",
      "269,0\n",
      "270,1\n",
      "271,1\n",
      "272,0\n",
      "273,0\n",
      "274,1\n",
      "275,1\n",
      "276,1\n",
      "277,1\n",
      "278,0\n",
      "279,0\n",
      "280,0\n",
      "281,0\n",
      "282,1\n",
      "283,1\n",
      "284,0\n",
      "285,1\n",
      "286,0\n",
      "287,0\n",
      "288,1\n",
      "289,1\n",
      "290,0\n",
      "291,0\n",
      "292,0\n",
      "293,1\n",
      "294,0\n",
      "295,1\n",
      "296,0\n",
      "297,1\n",
      "298,0\n",
      "299,0\n",
      "300,1\n",
      "301,1\n",
      "302,1\n",
      "303,1\n",
      "304,1\n",
      "305,0\n",
      "306,0\n",
      "307,1\n",
      "308,1\n",
      "309,1\n",
      "310,1\n",
      "311,1\n",
      "312,1\n",
      "313,0\n",
      "314,1\n",
      "315,1\n",
      "316,1\n",
      "317,0\n",
      "318,1\n",
      "319,1\n"
     ]
    }
   ],
   "source": [
    "dict_test_output={}\n",
    "\n",
    "for i in range(0,len(updated_test)):\n",
    "    if Perplexity(updated_test[i],truth_test_Unigrams_Probabilty,UnigramProbabilityforPerplexity) < Perplexity(updated_test[i],deceptive_test_Unigrams_Probability,UnigramProbabilityforPerplexity):\n",
    "       dict_test_output[i] = 0\n",
    "       print(str(i) + ',' + '0')\n",
    "    else:\n",
    "        dict_test_output[i] = 1\n",
    "        print(str(i) + ',' + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
